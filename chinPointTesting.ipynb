{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c25242ad-8e0d-48d0-8f87-2f305b7fcda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import time #for tracking\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class chinPointDataset(Dataset):\n",
    "    def __init__(self, imageFolder, annotFile, idealWidth, idealHeight):\n",
    "        if(annotFile):\n",
    "            self.imageFolder = imageFolder\n",
    "            self.annotFile = annotFile\n",
    "            t1=time.time()\n",
    "            # count images\n",
    "            self.size = 0\n",
    "            for image in os.listdir(imageFolder):\n",
    "                if image.endswith(\".png\"): #redundant\n",
    "                    self.size += 1\n",
    "            \n",
    "            # open json\n",
    "            with open(annotFile, 'r') as f:\n",
    "                self.jsonAnnot = json.load(f)\n",
    "            \n",
    "            # preallocate arrays\n",
    "            self.allImgsArray = np.zeros([self.size, idealHeight, idealWidth], dtype=np.float32)\n",
    "            self.allAnnotsArray = np.zeros([self.size, 2], dtype=np.float32)\n",
    "            \n",
    "            # load all images and annotations\n",
    "\n",
    "            # loop through and load all iamges in a folder. \n",
    "            #create a numpy array from these images.\n",
    "            #then recolor it and resize it and app\n",
    "            t2=time.time()\n",
    "            for idx, image in enumerate(os.listdir(imageFolder)):\n",
    "                if image.endswith(\".png\"): #redundant\n",
    "                    \n",
    "                    imgArr = cv2.imread(os.path.join(imageFolder, image))\n",
    "                    height, width, channels = imgArr.shape\n",
    "                    imgArr = cv2.cvtColor(imgArr, cv2.COLOR_BGR2GRAY)\n",
    "                    imgArr = cv2.resize(imgArr, (idealWidth, idealHeight))  # cv2.resize takes (width, height)\n",
    "                    \n",
    "                    # normalize to 0-1 - crucial because nns generally work best with 0-1 normalized data... \n",
    "                    self.allImgsArray[idx] = imgArr.astype(np.float32) / 255.0\n",
    "                    \n",
    "                    # scale annotation from original coords to ideal coords\n",
    "                    origX, origY = self.jsonAnnot[image]\n",
    "                    \n",
    "                    self.allAnnotsArray[idx] = [\n",
    "                        origX * (idealWidth / width),\n",
    "                        origY * (idealHeight / height)\n",
    "                    ]\n",
    "            \n",
    "        else:\n",
    "            t1=time.time()\n",
    "            self.size = len(os.listdir(imageFolder))\n",
    "            self.allImgsArray = np.zeros([self.size, idealHeight, idealWidth], dtype=np.float32)\n",
    "            t2=time.time()\n",
    "            for idx, image in enumerate(os.listdir(imageFolder)):\n",
    "                if image.endswith(\".png\"): #redundant\n",
    "                    imgArr = cv2.imread(os.path.join(imageFolder, image))\n",
    "                    height, width, channels = imgArr.shape\n",
    "                    imgArr = cv2.cvtColor(imgArr, cv2.COLOR_BGR2GRAY)\n",
    "                    imgArr = cv2.resize(imgArr, (idealWidth, idealHeight))  # cv2.resize takes (width, height)\n",
    "                    \n",
    "                    # normalize to 0-1 - crucial because nns generally work best with 0-1 normalized data... \n",
    "                    self.allImgsArray[idx] = imgArr.astype(np.float32) / 255.0\n",
    "        t3=time.time()\n",
    "        print(f\"Time taken for each stage:\")\n",
    "        print(f\"Time taken for counting & loading annotations: {t2-t1} seconds\")\n",
    "        print(f\"Time taken for loading & processing images: {t3-t2} seconds\")\n",
    "        print(f\"Total time taken: {t3-t1} seconds\")\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # returns (image, annotation) as tensors\n",
    "        img = torch.from_numpy(self.allImgsArray[idx]).unsqueeze(0)  # shape: [1, H, W] for single channel\n",
    "        annot = torch.from_numpy(self.allAnnotsArray[idx])  # shape: [2]\n",
    "        return img, annot\n",
    "\n",
    "class chinPointNet(nn.Module):\n",
    "    def __init__(self,ks=3):\n",
    "        super().__init__()\n",
    "        # simple conv stack -> regression head\n",
    "        # input: [batch, 1, 96, 96]\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=ks, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=ks, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=ks, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # after 3 pools: 96 -> 48 -> 24 -> 12\n",
    "        # so final feature map is [batch, 128, 12, 12]\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)  # output: [batch, 2] for (x, y)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cc773-e52a-407e-9e11-3f4784231ba6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (3990684854.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtestChinPointDataset = chinPointDataset(imageFolder, annotFile=None, idealWidth, idealHeight)\u001b[39m\n                                                                                                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "def lrScheduler(baseLr, i):\n",
    "    # formula: scales down, 10% of orig at 500 iters, 1% at 1000\n",
    "    return baseLr * (10 ** (-i / 500))\n",
    "\n",
    "imageFolder = \"Imgs\"\n",
    "annotFile = \"Imgs_Annotate.json\"\n",
    "testImageFolder = \"testImgs\"\n",
    "idealWidth = 96\n",
    "idealHeight = 96\n",
    "chinPointDataset = chinPointDataset(imageFolder, annotFile, idealWidth, idealHeight)\n",
    "testChinPointDataset = chinPointDataset(imageFolder, annotFile=None, idealWidth, idealHeight)\n",
    "dataloader = DataLoader(\n",
    "    chinPointDataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,              # âœ… Parallel data loading\n",
    "    pin_memory=True,            # âœ… CRITICAL for fast GPU transfer!\n",
    "    persistent_workers=False     # âœ… Reuse workers across epochs\n",
    ")\n",
    "testDataloader = DataLoader(\n",
    "    testChinPointDataset,\n",
    "    batch_size=36,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False\n",
    ")\n",
    "# Initialize model ON GPU\n",
    "model = chinPointNet().to(\"cuda\") \n",
    "\n",
    "# Loss function (can stay on CPU, will auto-move)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "baseLr = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=baseLr)  # Added momentum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dab053",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = 'Imgs'\n",
    "ANNOT_PATH = 'Imgs_Annotate.json'\n",
    "OUTPUT_PATH = 'trainPredPlots'\n",
    "BATCH_SIZE = 64\n",
    "def savePreds(preds, label, it, da, def_size=96,outputPath=OUTPUT_PATH):\n",
    "    # for keys, values in imgAnnot.items():\n",
    "    #     if keys == imgAnnot\n",
    "    global BATCH_SIZE\n",
    "    print('saving...')\n",
    "    data = da.to('cpu').squeeze(0).numpy()\n",
    "    print(data.shape)\n",
    "    data = data.transpose(0,2,3,1)\n",
    "    # plt.imshow(data[0])\n",
    "    # plt.plot()\n",
    "    # plt.show()\n",
    "    iter = it\n",
    "    print(\"PREDS:\\n\",preds)\n",
    "       # Multiply by def_size and round to nearest integer\n",
    "    Opreds = torch.round(preds)\n",
    "    if(label):\n",
    "        Olabel = torch.round(label)\n",
    "    else:\n",
    "        Olabel = None\n",
    "    #print(\"Lookie here: \\n\",Opreds, Olabel)\n",
    "    \n",
    "    # print(\"Rounded predictions:\", Opreds)\n",
    "    # print(\"Rounded labels:\", Olabel)\n",
    "\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(36, 36))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        fileName = f\"{i+iter:06ed}.png\"\n",
    "\n",
    "    # Directly extract and convert to float from tensors\n",
    "        xp, yp = Opreds[i][0].item(), Opreds[i][1].item()\n",
    "        if(label):\n",
    "            xl, yl = Olabel[i][0].item(), Olabel[i][1].item()\n",
    "        else:\n",
    "            xl, yl = None, None\n",
    "        # print(xp,yp)\n",
    "        # print(xl,yl)\n",
    "\n",
    "        # print(f\"Coordinates (Preds): x={xp}, y={yp}\")\n",
    "        # print(f\"Coordinates (Label): x={xl}, y={yl}\")\n",
    "        if xp <= 0:\n",
    "            xp = 0\n",
    "        if yp <= 0:\n",
    "            yp = 0\n",
    "        \n",
    "        #for file in os.listdir(imgPath):\n",
    "        #full_path = os.path.join(imgPath, fileName)\n",
    "        new_path = os.path.join(OUTPUT_PATH, fileName)\n",
    "        img = data[i]\n",
    "        ax.imshow(img,cmap='gray')\n",
    "        ax.axis('off')  # Turn off the axis for a cleaner look\n",
    "        ax.set_title(f'Image {i + 1}')  # Set the title for each subplot\n",
    "        \n",
    "        p = plt.Circle((xp, yp), 3, color='red')\n",
    "        if(label):\n",
    "            l = plt.Circle((xl, yl), 3, color='blue')\n",
    "            ax.add_patch(l)\n",
    "        ax.add_patch(p)\n",
    "\n",
    "    fileName = f\"{(iter*BATCH_SIZE):06d}_{((iter+1)*BATCH_SIZE):06d}.png\"\n",
    "    plt.tight_layout()\n",
    "    new_path = os.path.join(OUTPUT_PATH, fileName)\n",
    "    plt.savefig(new_path)\n",
    "\n",
    "    #plt.show()\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "850a9d53",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (1233770962.py, line 30)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31msavePreds(testOutAnnot,label=None,epoch,testImg,outputPath=\"testPredPlots\")\u001b[39m\n                                                                              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.amp.GradScaler(\"cuda\")  # GradScaler for CUDA AMP\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # training loop\n",
    "    for epoch in range(1000):\n",
    "        for batchIdx, (batchImg, batchGTAnnot) in enumerate(dataloader):\n",
    "            # forward\n",
    "            batchImg = batchImg.to(\"cuda\", non_blocking=True)\n",
    "            batchGTAnnot = batchGTAnnot.to(\"cuda\", non_blocking=True)\n",
    "            batchOutAnnot = model(batchImg)\n",
    "            loss = criterion(batchOutAnnot, batchGTAnnot)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                batchOutAnnot = model(batchImg)\n",
    "                loss = criterion(batchOutAnnot, batchGTAnnot)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        newLr = lrScheduler(baseLr, epoch)\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = newLr\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, LR: {newLr:.6f}\")\n",
    "            savePreds(batchOutAnnot,batchGTAnnot,epoch,batchImg)\n",
    "            testImg = next(iter(testDataloader))\n",
    "            testImg = testImg.to(\"cuda\", non_blocking=True)\n",
    "            testOutAnnot = model(testImg)\n",
    "            savePreds(testOutAnnot,label=None,epoch,testImg,outputPath=\"testPredPlots\")\n",
    "        if epoch % 50 == 0:\n",
    "            torch.save(model.state_dict(), f\"modelCheckpoints/epoch_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4c33bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === TESTING chinPointDataset.__getitem__() ===\n",
    "\n",
    "# print(\"=\"*70)\n",
    "# print(\"TESTING DATASET __getitem__ METHOD\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Method 1: Using bracket notation (preferred)\n",
    "# print(\"\\n--- Method 1: Bracket notation ---\")\n",
    "# img, annot = chinPointDataset[0]  # This calls __getitem__(0)\n",
    "# print(f\"Image shape: {img.shape}\")\n",
    "# print(f\"Image dtype: {img.dtype}\")\n",
    "# print(f\"Image min/max values: {img.min():.3f} / {img.max():.3f}\")\n",
    "# print(f\"Annotation: {annot}\")\n",
    "# print(f\"Annotation shape: {annot.shape}\")\n",
    "\n",
    "# # Method 2: Calling __getitem__ directly (less common)\n",
    "# print(\"\\n--- Method 2: Direct call (equivalent) ---\")\n",
    "# img2, annot2 = chinPointDataset.__getitem__(0)\n",
    "# print(f\"Same result: {torch.equal(img, img2)}\")\n",
    "\n",
    "# # Test multiple samples\n",
    "# print(\"\\n--- Testing multiple samples ---\")\n",
    "# for i in range(min(5, len(chinPointDataset))):\n",
    "#     img, annot = chinPointDataset[i]\n",
    "#     print(f\"Sample {i}: img shape={img.shape}, annot={annot.numpy()}\")\n",
    "\n",
    "# # Test random samples\n",
    "# print(\"\\n--- Testing random samples ---\")\n",
    "# for _ in range(3):\n",
    "#     idx = random.randint(0, len(chinPointDataset) - 1)\n",
    "#     img, annot = chinPointDataset[idx]\n",
    "#     print(f\"Random idx {idx}: annot point = ({annot[0]:.2f}, {annot[1]:.2f})\")\n",
    "\n",
    "\n",
    "\n",
    "# # Visualize a sample\n",
    "# print(\"\\n--- Visualizing sample 0 ---\")\n",
    "# img, annot = chinPointDataset[0]\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(img.squeeze(), cmap='gray')\n",
    "# plt.scatter(annot[0].item(), annot[1].item(), c='red', s=100, marker='x', linewidths=3)\n",
    "# plt.title(f'Sample 0: Chin Point at ({annot[0]:.1f}, {annot[1]:.1f})')\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"\\nTotal dataset size: {len(chinPointDataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f59af247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# =====================================================================\n",
    "# PYTORCH DATALOADER COMPREHENSIVE TUTORIAL\n",
    "# Web-backed best practices for 2025\n",
    "# =====================================================================\n",
    "# \"\"\"\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# import time\n",
    "\n",
    "# print(\"=\"*70)\n",
    "# print(\"PART 1: BASIC DATALOADER USAGE\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Create a basic DataLoader\n",
    "# basicLoader = DataLoader(\n",
    "#     dataset=chinPointDataset,\n",
    "#     batch_size=32,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# print(f\"\\nDataLoader created!\")\n",
    "# print(f\"Total samples: {len(chinPointDataset)}\")\n",
    "# print(f\"Batch size: {basicLoader.batch_size}\")\n",
    "# print(f\"Number of batches: {len(basicLoader)}\")\n",
    "\n",
    "# # Iterate through the DataLoader\n",
    "# print(\"\\n--- First 3 batches ---\")\n",
    "# for batchIdx, (images, annotations) in enumerate(basicLoader):\n",
    "#     if batchIdx < 3:\n",
    "#         print(f\"Batch {batchIdx}:\")\n",
    "#         print(f\"  Images shape: {images.shape}\")  # [batch_size, 1, H, W]\n",
    "#         print(f\"  Annotations shape: {annotations.shape}\")  # [batch_size, 2]\n",
    "#         print(f\"  First annotation in batch: {annotations[0]}\")\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"PART 2: KEY DATALOADER PARAMETERS\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # 2.1 batch_size\n",
    "# print(\"\\n--- 2.1 BATCH SIZE ---\")\n",
    "# print(\"Controls how many samples per batch\")\n",
    "# print(\"Larger = faster training but more memory\")\n",
    "# print(\"Common values: 16, 32, 64, 128, 256\")\n",
    "\n",
    "# loaderSmallBatch = DataLoader(chinPointDataset, batch_size=8)\n",
    "# loaderLargeBatch = DataLoader(chinPointDataset, batch_size=64)\n",
    "# print(f\"Small batch (8): {len(loaderSmallBatch)} batches\")\n",
    "# print(f\"Large batch (64): {len(loaderLargeBatch)} batches\")\n",
    "\n",
    "# # 2.2 shuffle\n",
    "# print(\"\\n--- 2.2 SHUFFLE ---\")\n",
    "# print(\"Randomizes sample order each epoch\")\n",
    "# print(\"CRITICAL for training! Prevents learning data order\")\n",
    "\n",
    "# loaderNoShuffle = DataLoader(chinPointDataset, batch_size=4, shuffle=False)\n",
    "# loaderWithShuffle = DataLoader(chinPointDataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# print(\"\\nNo shuffle (first batch indices in order):\")\n",
    "# for i, (img, annot) in enumerate(loaderNoShuffle):\n",
    "#     if i == 0:\n",
    "#         print(f\"  First sample annot: {annot[0]}\")\n",
    "#     break\n",
    "\n",
    "# print(\"\\nWith shuffle (randomized):\")\n",
    "# for i, (img, annot) in enumerate(loaderWithShuffle):\n",
    "#     if i == 0:\n",
    "#         print(f\"  First sample annot: {annot[0]}\")\n",
    "#     break\n",
    "\n",
    "# # 2.3 num_workers\n",
    "# print(\"\\n--- 2.3 NUM_WORKERS (Parallel Data Loading) ---\")\n",
    "# print(\"Uses multiple processes to load data faster\")\n",
    "# print(\"Best Practice: Start with 4, increase if I/O bound\")\n",
    "# print(\"Warning: On Windows, num_workers can cause issues\")\n",
    "# print(\"         Use 0 for debugging, 2-4 for training\")\n",
    "\n",
    "# # Test with different worker counts\n",
    "# batchSize = 32\n",
    "# testSize = min(len(chinPointDataset), 500)\n",
    "\n",
    "# for numWorkers in [0, 2]:\n",
    "#     testLoader = DataLoader(\n",
    "#         chinPointDataset,\n",
    "#         batch_size=batchSize,\n",
    "#         shuffle=True,\n",
    "#         num_workers=numWorkers\n",
    "#     )\n",
    "    \n",
    "#     start = time.time()\n",
    "#     for i, (img, annot) in enumerate(testLoader):\n",
    "#         if i >= testSize // batchSize:\n",
    "#             break\n",
    "#     elapsed = time.time() - start\n",
    "    \n",
    "#     print(f\"num_workers={numWorkers}: {elapsed:.3f}s for {testSize} samples\")\n",
    "\n",
    "# # 2.4 drop_last\n",
    "# print(\"\\n--- 2.4 DROP_LAST ---\")\n",
    "# print(\"Drops the last incomplete batch if dataset size not divisible by batch_size\")\n",
    "\n",
    "# loaderKeepLast = DataLoader(chinPointDataset, batch_size=32, drop_last=False)\n",
    "# loaderDropLast = DataLoader(chinPointDataset, batch_size=32, drop_last=True)\n",
    "\n",
    "# print(f\"Dataset size: {len(chinPointDataset)}\")\n",
    "# print(f\"Keep last: {len(loaderKeepLast)} batches\")\n",
    "# print(f\"Drop last: {len(loaderDropLast)} batches\")\n",
    "\n",
    "# # 2.5 pin_memory\n",
    "# print(\"\\n--- 2.5 PIN_MEMORY ---\")\n",
    "# print(\"Speeds up CPUâ†’GPU transfer\")\n",
    "# print(\"Set to True if using CUDA\")\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Current device: {device}\")\n",
    "\n",
    "# loaderPinned = DataLoader(\n",
    "#     chinPointDataset,\n",
    "#     batch_size=32,\n",
    "#     pin_memory=True if torch.cuda.is_available() else False\n",
    "# )\n",
    "# print(f\"Pin memory enabled: {loaderPinned.pin_memory}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"PART 3: REAL TRAINING SCENARIO\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Split dataset into train/val (80/20 split)\n",
    "# datasetSize = len(chinPointDataset)\n",
    "# trainSize = int(0.8 * datasetSize)\n",
    "# valSize = datasetSize - trainSize\n",
    "\n",
    "# print(f\"\\nDataset split:\")\n",
    "# print(f\"  Total: {datasetSize}\")\n",
    "# print(f\"  Train: {trainSize} ({trainSize/datasetSize*100:.1f}%)\")\n",
    "# print(f\"  Val: {valSize} ({valSize/datasetSize*100:.1f}%)\")\n",
    "\n",
    "# # Use random_split to split dataset\n",
    "# from torch.utils.data import random_split\n",
    "\n",
    "# trainDataset, valDataset = random_split(\n",
    "#     chinPointDataset,\n",
    "#     [trainSize, valSize],\n",
    "#     generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    "# )\n",
    "\n",
    "# # Create DataLoaders with best practices\n",
    "# trainLoader = DataLoader(\n",
    "#     trainDataset,\n",
    "#     batch_size=32,\n",
    "#     shuffle=True,          # IMPORTANT: Shuffle training data\n",
    "#     num_workers=2,         # Parallel loading\n",
    "#     pin_memory=torch.cuda.is_available(),  # Fast GPU transfer\n",
    "#     drop_last=True         # Consistent batch sizes\n",
    "# )\n",
    "\n",
    "# valLoader = DataLoader(\n",
    "#     valDataset,\n",
    "#     batch_size=64,          # Can use larger batch for validation\n",
    "#     shuffle=False,          # NO shuffle for validation\n",
    "#     num_workers=2,\n",
    "#     pin_memory=torch.cuda.is_available(),\n",
    "#     drop_last=False         # Keep all validation data\n",
    "# )\n",
    "\n",
    "# print(f\"\\nTrain loader: {len(trainLoader)} batches\")\n",
    "# print(f\"Val loader: {len(valLoader)} batches\")\n",
    "\n",
    "# # Simulate training loop\n",
    "# print(\"\\n--- Simulated training loop ---\")\n",
    "# numEpochs = 2\n",
    "\n",
    "# for epoch in range(numEpochs):\n",
    "#     print(f\"\\nEpoch {epoch+1}/{numEpochs}\")\n",
    "    \n",
    "#     # Training\n",
    "#     trainLoss = 0\n",
    "#     for batchIdx, (images, annotations) in enumerate(trainLoader):\n",
    "#         # Move to device (GPU if available)\n",
    "#         images = images.to(device)\n",
    "#         annotations = annotations.to(device)\n",
    "        \n",
    "#         # Your model forward pass would go here\n",
    "#         # loss = model(images, annotations)\n",
    "        \n",
    "#         if batchIdx < 2:  # Print first 2 batches\n",
    "#             print(f\"  Train batch {batchIdx}: images {images.shape}, annots {annotations.shape}\")\n",
    "    \n",
    "#     # Validation\n",
    "#     with torch.no_grad():\n",
    "#         valLoss = 0\n",
    "#         for batchIdx, (images, annotations) in enumerate(valLoader):\n",
    "#             images = images.to(device)\n",
    "#             annotations = annotations.to(device)\n",
    "            \n",
    "#             # Your model evaluation would go here\n",
    "            \n",
    "#             if batchIdx < 2:\n",
    "#                 print(f\"  Val batch {batchIdx}: images {images.shape}, annots {annotations.shape}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"PART 4: ADVANCED TIPS & BEST PRACTICES\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# print(\"\"\"\n",
    "# ðŸŽ¯ BEST PRACTICES SUMMARY (2025):\n",
    "\n",
    "# 1. BATCH SIZE:\n",
    "#    - Start with 32, increase if you have GPU memory\n",
    "#    - Powers of 2 (16, 32, 64, 128) often work best\n",
    "#    - Smaller batches = noisier gradients but better generalization\n",
    "\n",
    "# 2. SHUFFLE:\n",
    "#    - ALWAYS True for training\n",
    "#    - ALWAYS False for validation/testing\n",
    "#    - Prevents model from learning data order\n",
    "\n",
    "# 3. NUM_WORKERS:\n",
    "#    - Windows: Use 0 for debugging, 2-4 for training\n",
    "#    - Linux/Mac: Can use more (4-8)\n",
    "#    - Too many workers = slower due to overhead\n",
    "#    - Test different values with your specific dataset\n",
    "\n",
    "# 4. PIN_MEMORY:\n",
    "#    - Set to True if using GPU\n",
    "#    - Speeds up hostâ†’device transfer\n",
    "#    - Uses more CPU memory\n",
    "\n",
    "# 5. DROP_LAST:\n",
    "#    - True for training (consistent batch sizes help some models)\n",
    "#    - False for validation (don't lose data)\n",
    "\n",
    "# 6. PERSISTENT_WORKERS (Advanced):\n",
    "#    - Set to True to reuse workers across epochs\n",
    "#    - Faster but uses more memory\n",
    "#    - Only works if num_workers > 0\n",
    "\n",
    "# 7. PREFETCH_FACTOR (Advanced):\n",
    "#    - Controls how many batches each worker preloads\n",
    "#    - Default is 2 (good for most cases)\n",
    "\n",
    "# âš¡ PERFORMANCE TIPS:\n",
    "# - Preload all data into memory if possible (like your dataset does!)\n",
    "# - Use transforms in __getitem__ for on-the-fly augmentation\n",
    "# - Profile your code to find bottlenecks\n",
    "# - Consider using torch.compile() for the model (PyTorch 2.0+)\n",
    "\n",
    "# âš ï¸ COMMON MISTAKES:\n",
    "# âŒ Shuffling validation data\n",
    "# âŒ Different batch sizes between epochs\n",
    "# âŒ Not using pin_memory with GPU\n",
    "# âŒ Too many num_workers causing overhead\n",
    "# âŒ Not dropping last batch when model requires fixed batch size\n",
    "\n",
    "# âœ… TYPICAL CONFIGURATION:\n",
    "# \"\"\")\n",
    "\n",
    "# print(\"\\n# For TRAINING:\")\n",
    "# print(\"\"\"trainLoader = DataLoader(\n",
    "#     dataset=trainDataset,\n",
    "#     batch_size=32,\n",
    "#     shuffle=True,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=torch.cuda.is_available(),\n",
    "#     drop_last=True\n",
    "# )\"\"\")\n",
    "\n",
    "# print(\"\\n# For VALIDATION/TESTING:\")\n",
    "# print(\"\"\"valLoader = DataLoader(\n",
    "#     dataset=valDataset,\n",
    "#     batch_size=64,\n",
    "#     shuffle=False,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=torch.cuda.is_available(),\n",
    "#     drop_last=False\n",
    "# )\"\"\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"DATALOADER TUTORIAL COMPLETE!\")\n",
    "# print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0516df7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "827dc065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.pop(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a77d214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7c414b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'insert'"
     ]
    }
   ],
   "source": [
    "d[]\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be5b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How dictionaries work in python:\n",
    "\n",
    "#d.update() #overrides values and inserts new ones whereever they do not yet exist. Does it do this regardless of order?\n",
    "dict = {\"a\":1, \"b\":2,\"ca\":4}\n",
    "dict.get(\"key\") #returns none if key is DNE, else its value.\n",
    "dict.get(\"item\",1) #returns returnvalue (1 is current returnvalue) if key is DNE, else the key's value.\n",
    "#enumerate() splits a list or dict into an index and the value.. by default for a dictionary this results in, if idx,item is done, the item being the key.\n",
    "#just doing dict.copy() passes pointers to the data to a new instance allocated in memory, aka a shallow copy. But deep copying actually copies the \n",
    "# data itself to a new object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a60c19d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(dict.get(\"ca\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "31087cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11881376\n"
     ]
    }
   ],
   "source": [
    "d.get('z', 1.3**7)\n",
    "print(26**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9c2c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  and: a\n",
      "1  and: b\n",
      "2  and: ca\n"
     ]
    }
   ],
   "source": [
    "l=[5,6,44,1]\n",
    "for idx,f in enumerate(dict):\n",
    "    print(idx,\" and: \" + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637864f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-ai-stack-26 (venv)",
   "language": "python",
   "name": "llms-ai-stack-26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
